"""
Multi-Agent Analyst Module
Orchestrates a debate between multiple AI agents with different personas to analyze a stock.
"""
import asyncio
import json
import logging
from typing import Dict, Any, List, AsyncGenerator
from datetime import datetime

# Import low-level API callers from llm_analyst
from llm_analyst import generate_analysis_openai, generate_analysis_gemini

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StockAnalystAgent:
    """
    Represents a single specialized analyst agent.
    """
    def __init__(self, name: str, role: str, description: str, system_prompt: str):
        self.name = name
        self.role = role
        self.description = description
        self.system_prompt = system_prompt

    async def analyze(self, context: str, api_config: Dict[str, Any]) -> str:
        """
        Perform analysis based on the agent's persona.
        """
        prompt = f"""
è¯·ä½ æ‰®æ¼”ã€{self.name}ã€‘ï¼ˆ{self.role}ï¼‰ã€‚
ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯ï¼š{self.description}

{context}

è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œç»™å‡ºä½ çš„ä¸“ä¸šåˆ†ææ„è§ã€‚
è¦æ±‚ï¼š
1. ä¸¥æ ¼éµå®ˆä½ çš„äººè®¾ï¼Œä¸è¦è¯•å›¾å¹³è¡¡è§‚ç‚¹ï¼Œé‚£æ˜¯CIOçš„å·¥ä½œã€‚
2. è§‚ç‚¹å¿…é¡»é²œæ˜ï¼Œæœ‰ç†æœ‰æ®ã€‚
3. å¦‚æœæ•°æ®ä¸è¶³ä»¥æ”¯æŒä½ çš„é¢†åŸŸåˆ†æï¼Œç›´æ¥æŒ‡å‡ºã€‚
4. è¾“å‡ºæ ¼å¼ä¸ºMarkdownï¼Œä¸è¦åŒ…å«å¯’æš„ã€‚
"""
        # Log the full prompt
        print(f"\n======== [Agent Prompt Debug: {self.name}] ========\n{prompt}\n===================================================\n")

        # Call LLM
        # We construct a fake stock_info/tech_data to satisfy the function signature if we reuse llm_analyst, 
        # OR we call the low-level functions directly. 
        # Calling low-level functions is better.
        
        provider = api_config.get('provider', 'openai')
        
        try:
            if provider == 'gemini':
                # Map config for Gemini
                response = generate_analysis_gemini(
                    stock_info={},  # Not used directly if we override logic, but wait, the low level func builds prompt.
                    # We might need to call client directly if we want custom prompts.
                    # Let's bypass generate_analysis_gemini's prompt building if possible
                    # checking llm_analyst.py... create_analysis_prompt is called inside.
                    # This implies we cannot easily reuse generate_analysis_gemini for custom prompts without modification.
                    # To avoid modifying llm_analyst heavily, I will implement a simple direct caller here based on llm_analyst's implementation.
                    tech_data={}, 
                    project_id=api_config['project_id'],
                    location=api_config['location'],
                    credentials_path=api_config.get('credentials_path'),
                    model=api_config.get('model', 'gemini-2.5-flash'),
                    analysis_type="custom_agent", # Hack: We need to handle this in llm_analyst OR implement call here.
                    realtime_data=None 
                )
                # Wait, generate_analysis_gemini calls create_analysis_prompt inside.
                # If analysis_type is unknown, create_analysis_prompt might fail or return default.
                # It's better to implement a simple_call_llm here.
                return await self._call_llm_direct(prompt, api_config)

            else:
                return await self._call_llm_direct(prompt, api_config)
                
        except Exception as e:
            logger.error(f"Agent {self.name} failed: {e}")
            return f"**{self.name} åˆ†æå¤±è´¥**: {str(e)}"

    async def _call_llm_direct(self, prompt: str, api_config: Dict[str, Any]) -> str:
        """
        Direct LLM call bypassing llm_analyst's specific prompt construction logic.
        Supports OpenAI and Gemini.
        """
        provider = api_config.get('provider', 'openai')
        
        if provider == 'gemini':
            # Gemini Implementation
            try:
                # Lazy import to avoid dependency issues if not installed
                from google import genai
                from google.genai import types
                import os
                
                if api_config.get('credentials_path') and os.path.exists(api_config['credentials_path']):
                    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = api_config['credentials_path']
                
                client = genai.Client(
                    vertexai=True,
                    project=api_config['project_id'],
                    location=api_config['location']
                )
                
                response = client.models.generate_content(
                    model=api_config.get('model', 'gemini-2.5-flash'),
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.3,
                        max_output_tokens=4096,
                        system_instruction=self.system_prompt
                    )
                )
                
                if hasattr(response, 'text'):
                    return response.text
                elif hasattr(response, 'candidates') and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        return ''.join([part.text for part in candidate.content.parts if hasattr(part, 'text')])
                return str(response)
                
            except ImportError:
                return "Google Gen AI SDK not installed."
            except Exception as e:
                return f"Gemini API Error: {str(e)}"

        else:
            # OpenAI / DeepSeek / GLM Implementation
            try:
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=api_config['api_key'],
                    base_url=api_config['base_url']
                )
                
                api_params = {
                    "model": api_config['model'],
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 4096
                }
                
                # Special handling for GLM thinking mode disable
                if provider == "glm":
                    api_params["extra_body"] = {"thinking": {"type": "disabled"}}

                response = client.chat.completions.create(**api_params)
                return response.choices[0].message.content
                
            except Exception as e:
                return f"OpenAI/Compatible API Error: {str(e)}"


class MultiAgentSystem:
    def __init__(self, api_config: Dict[str, Any]):
        self.api_config = api_config
        self.agents = [
            StockAnalystAgent(
                name="æŠ€æœ¯æ´¾ (Technician)",
                role="æŠ€æœ¯åˆ†æä¸“å®¶",
                description="ä½ æ˜¯ä¸€åçº¯ç²¹çš„æŠ€æœ¯åˆ†æå¸ˆã€‚ä½ åªç›¸ä¿¡å›¾å½¢ã€è¶‹åŠ¿ã€å‡çº¿ï¼ˆMAï¼‰ã€é‡ä»·é…åˆï¼ˆVolumeï¼‰å’ŒåŠ¨é‡æŒ‡æ ‡ï¼ˆRSI/MACDï¼‰ã€‚",
                system_prompt="ä½ æ˜¯ä¸€åä¸¥è°¨çš„æŠ€æœ¯åˆ†æå¸ˆã€‚æˆ‘ä¸å…³å¿ƒåŸºæœ¬é¢ï¼Œä¹Ÿä¸å…³å¿ƒå®è§‚æ–°é—»ã€‚æˆ‘åªçœ‹ä»·æ ¼è¡Œä¸º(Price Action)ã€‚å¦‚æœä»·æ ¼è·Œç ´å‡çº¿ï¼Œå°±æ˜¯å–å‡ºä¿¡å·ã€‚å¦‚æœæ”¾é‡çªç ´ï¼Œå°±æ˜¯ä¹°å…¥ä¿¡å·ã€‚"
            ),
            StockAnalystAgent(
                name="é£æ§å®˜ (Risk Officer)",
                role="é£é™©æ§åˆ¶ä¸“å®¶",
                description="ä½ æ˜¯å›¢é˜Ÿä¸­çš„åˆ¹è½¦ç‰‡ã€‚ä½ æåº¦åŒæ¶é£é™©ã€‚ä½ å…³æ³¨æ³¢åŠ¨ç‡ï¼ˆATRï¼‰ã€æœ€å¤§å›æ’¤ã€ç›ˆäºæ¯”ï¼ˆR:Rï¼‰ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯»æ‰¾ä»»ä½•å¯èƒ½å¯¼è‡´äºæŸçš„ç†ç”±ã€‚",
                system_prompt="ä½ æ˜¯ä¸€åè‹›åˆ»çš„é£é™©æ§åˆ¶å®˜ã€‚ä½ çš„èŒè´£æ˜¯æ³¼å†·æ°´ã€‚æˆ‘ä»¬è¦ä¿æŠ¤æœ¬é‡‘ã€‚ä»»ä½•æœªç»ç¡®è®¤çš„ä¸Šæ¶¨éƒ½æ˜¯è¯±å¤šã€‚ä»»ä½•æŒ‡æ ‡èƒŒç¦»éƒ½æ˜¯é™·é˜±ã€‚ä½ è¦æŒ‡å‡ºæœ€åçš„æƒ…å†µã€‚"
            ),
            StockAnalystAgent(
                name="åŸºæœ¬é¢ (Fundamentalist)",
                role="åŸºæœ¬é¢ä¸é€»è¾‘åˆ†æå¸ˆ",
                description="ä½ å…³æ³¨èµ„äº§èƒŒåçš„é€»è¾‘ã€‚å¦‚æœæ˜¯è‚¡ç¥¨ï¼Œä½ å…³æ³¨é¢˜æã€ä¸šç»©ã€æ–°é—»å‚¬åŒ–å‰‚ã€‚å¦‚æœæ˜¯ETFï¼Œä½ å…³æ³¨è¡Œä¸šå‘¨æœŸã€‚å¦‚æœæ˜¯Crypto/æœŸè´§ï¼Œä½ å…³æ³¨å®è§‚æƒ…ç»ªã€‚",
                system_prompt="ä½ æ˜¯ä¸€åå…·æœ‰å¤§å±€è§‚çš„ç ”ç©¶å‘˜ã€‚ä½ å…³æ³¨é•¿æœŸé€»è¾‘å’Œå¸‚åœºå™äº‹(Narrative)ã€‚å¿½ç•¥çŸ­æœŸçš„Kçº¿å™ªéŸ³ï¼Œå¯»æ‰¾é©±åŠ¨ä»·æ ¼ä¸Šæ¶¨çš„æ ¸å¿ƒé€»è¾‘ã€‚"
            )
        ]
        self.cio = StockAnalystAgent(
            name="CIO (é¦–å¸­æŠ•èµ„å®˜)",
            role="å†³ç­–è€…",
            description="ä½ æ˜¯æœ€ç»ˆå†³ç­–è€…ã€‚ä½ éœ€è¦ç»¼åˆå„æ–¹ä¸“å®¶çš„æ„è§ï¼Œåšå‡ºæœ€ç»ˆçš„ä¹°å–è£å†³ã€‚",
            system_prompt="ä½ æ˜¯ä¸€åªåŸºé‡‘çš„é¦–å¸­æŠ•èµ„å®˜ã€‚ä½ éœ€è¦å¬å–æŠ€æœ¯æ´¾ã€é£æ§å®˜å’ŒåŸºæœ¬é¢ç ”ç©¶å‘˜çš„è¾©è®ºã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š1. æ€»ç»“å„æ–¹è§‚ç‚¹ã€‚ 2. å¹³è¡¡æ”¶ç›Šä¸é£é™©ã€‚ 3. ç»™å‡ºæœ€ç»ˆçš„ã€æ˜ç¡®çš„æ“ä½œæŒ‡ä»¤ï¼ˆä¹°å…¥/æŒæœ‰/å‡ä»“/ç©ºä»“ï¼‰ã€‚ 4. åˆ¶å®šäº¤æ˜“è®¡åˆ’ï¼ˆä»“ä½ã€æ­¢æŸä½ï¼‰ã€‚ä¸è¦æ¨¡æ£±ä¸¤å¯ã€‚"
        )

    async def run_debate_stream(self, stock_info: Dict[str, Any], tech_data: Dict[str, Any], realtime_data: Dict[str, Any], start_progress: int = 30) -> AsyncGenerator[str, None]:
        """
        Run the debate and yield SSE events.
        Format: JSON string for SSE data.
        """
        # 1. Prepare Context
        asset_type = stock_info.get('asset_type', 'stock')
        
        # Fix Price 0 issue: Prioritize realtime, fallback to history close if 0 or None
        price = realtime_data.get('price')
        
        # Ensure we treat 0.0 as invalid
        if not price or float(price) == 0:
             price = tech_data.get('close', 0)
        
        # If still 0, try fallbacks
        if not price or float(price) == 0:
             # Try other potential keys
             price = tech_data.get('realtime_price', 0)

        # Last resort: Use MA5 or MA20 as proxy if price is completely missing but indicators exist
        # This prevents "Price: 0.0" which confuses LLM
        if (not price or float(price) == 0):
             if tech_data.get('ma5'):
                 price = tech_data.get('ma5')
             elif tech_data.get('ma20'):
                 price = tech_data.get('ma20')

        # Fix Date issue: Provide explicit date
        current_date = datetime.now().strftime('%Y-%m-%d')
        
        # è·å–Kçº¿å½¢æ€
        pattern_score = tech_data.get('pattern_score', 0)
        pattern_details = tech_data.get('pattern_details', [])
        pattern_str = "æ— "
        if pattern_details:
             pattern_str = ", ".join(pattern_details) + f" (ä¿®æ­£åˆ†: {pattern_score})"

        context = f"""
**åˆ†æå¯¹è±¡**ï¼š{stock_info['name']} ({stock_info['symbol']}) [{asset_type.upper()}]
**åˆ†ææ—¥æœŸ**ï¼š{current_date}
**å½“å‰ä»·æ ¼**ï¼š{price} (æ¶¨è·Œ: {realtime_data.get('change_pct', 0)}%)

**æŠ€æœ¯æŒ‡æ ‡æ¦‚è§ˆ**ï¼š
- MA20: {tech_data.get('ma20', 'N/A')}
- MA60: {tech_data.get('ma60', 'N/A')}
- RSI: {tech_data.get('rsi', 'N/A')}
- Kçº¿å½¢æ€: {pattern_str}
- é‡æ¯”: {realtime_data.get('volume_ratio', 'N/A')}
- æ³¢åŠ¨ç‡(ATR%): {tech_data.get('atr_pct', 'N/A')}%

**å¸‚åœºç¯å¢ƒ**ï¼š
- å¤§ç›˜æŒ‡æ•°ï¼š{realtime_data.get('market_index_price', 'N/A')} ({realtime_data.get('market_index_change', 0)}%)
"""
        yield json.dumps({"type": "progress", "value": start_progress, "message": "åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“è¾©è®ºç¯å¢ƒ..."})
        yield json.dumps({"type": "step", "content": "ğŸ”” è¾©è®ºç»„å»ºå®Œæ¯•ï¼Œå‡†å¤‡å¼€å§‹..."})
        
        # We start with the CIO section placeholder or header
        yield json.dumps({"type": "token", "content": "\n\n# ğŸ¤– AI ä¸“å®¶å›¢é˜Ÿè¾©è®ºçºªè¦\n\n"})
        
        agent_results = []
        
        # 2. Round 1: Parallel Analysis
        tasks = []
        total_agents = len(self.agents)
        
        # Send initial progress for analysis start
        current_progress = start_progress + 5
        yield json.dumps({"type": "progress", "value": current_progress, "message": "ä¸“å®¶å›¢é˜Ÿå¼€å§‹å¹¶è¡Œåˆ†æ..."})

        for i, agent in enumerate(self.agents):
            tasks.append(agent.analyze(context, self.api_config))
        
        # Wait for all (Parallel)
        results = await asyncio.gather(*tasks)
        
        debate_content = ""
        
        # Allocate 40% of progress bar for agents analysis (e.g., 35% -> 75%)
        # But allow some room for CIO. Let's say agents take us to 80%.
        # If start is 35, remaining is 65.
        # Agents phase: 35 -> 80 (delta 45)
        # CIO phase: 80 -> 95 (delta 15)
        
        progress_range_agents = 45
        
        # Process results with incremental progress updates
        for i, res in enumerate(results):
            agent = self.agents[i]
            # Calculate incremental progress
            inc = int(((i + 1) / total_agents) * progress_range_agents)
            progress_pct = current_progress + inc
            
            yield json.dumps({"type": "progress", "value": progress_pct, "message": f"{agent.name} å®Œæˆåˆ†æ"})
            yield json.dumps({"type": "step", "content": f"âœ… {agent.name} æäº¤äº†åˆ†ææŠ¥å‘Š"})

            # Format: Use HTML <details> for cleaner UI, so it's not one huge text block
            # But the user also wants to see it.
            # Let's use a nice blockquote or custom div structure if markdown supports it.
            # Using blockquote `> ` is standard.
            
            section_header = f"### ğŸ‘¤ {agent.name}\n"
            section_body = f"{res}\n\n"
            
            # Wrap in a way that looks like a card in Markdown?
            # We can use HTML directly since we render HTML.
            section_html_wrapper = f"""
<div class="agent-card mb-4 p-4 bg-gray-800/50 rounded-lg border border-gray-700">
    <div class="font-bold text-indigo-300 mb-2 border-b border-gray-700 pb-2">ğŸ‘¤ {agent.name}</div>
    <div class="prose prose-sm prose-invert text-gray-300">
{res}
    </div>
</div>
"""
            # NOTE: If we yield HTML directly as 'token', the frontend accumulating markdown might act weird
            # if it expects pure markdown.
            # However, standard Markdown parsers handle HTML blocks fine.
            # Let's try to stick to Markdown for safety but use quoted blocks.
            
            # ä½¿ç”¨ HTML details/summary å®ç°æŠ˜å æ•ˆæœ
            section_html = f"""
<details class="mb-3 group border border-gray-700/50 rounded-lg bg-gray-800/30 overflow-hidden">
    <summary class="cursor-pointer p-3 hover:bg-white/5 transition-colors flex items-center justify-between select-none list-none text-sm outline-none">
        <div class="flex items-center gap-2 font-bold text-indigo-300">
            <span>ğŸ‘¤</span>
            <span>{agent.name} åˆ†ææŠ¥å‘Š</span>
        </div>
        <span class="text-xs text-gray-500 transition-transform duration-200 group-open:rotate-180">â–¼</span>
    </summary>
    <div class="p-4 pt-2 border-t border-dashed border-gray-700/50 text-sm text-gray-300 leading-relaxed font-sans mt-2">
{res.replace(chr(10), '<br/>')}
    </div>
</details>
<div class="h-2"></div>
"""
            # ä¸ºäº†å†…å®¹å®Œæ•´æ€§ï¼Œdebate_content ç´¯åŠ  HTML
            debate_content += section_html
            
            # Stream the agent's output
            yield json.dumps({"type": "token", "content": section_html})
            agent_results.append(f"ã€{agent.name}æ„è§ã€‘:\n{res}")

        # 3. Round 2: CIO Decision
        yield json.dumps({"type": "step", "content": "ğŸ¤” é¦–å¸­æŠ•èµ„å®˜ (CIO) æ­£åœ¨æ±‡æ€»ä¸“å®¶æ„è§..."})
        yield json.dumps({"type": "progress", "value": 85, "message": "é¦–å¸­æŠ•èµ„å®˜ (CIO) æ­£åœ¨åˆ¶å®šæœ€ç»ˆå†³ç­–..."})
        
        cio_context = f"""
{context}

ä»¥ä¸‹æ˜¯å„ä½ä¸“å®¶çš„æ„è§ï¼š
{''.join(agent_results)}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¿›è¡Œæœ€ç»ˆæ€»ç»“å’Œå†³ç­–ã€‚
"""
        cio_result = await self.cio.analyze(cio_context, self.api_config)
        
        yield json.dumps({"type": "progress", "value": 95, "message": "æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."})
        yield json.dumps({"type": "step", "content": "âœï¸ CIO æ­£åœ¨ç­¾ç½²æœ€ç»ˆè£å†³ä¹¦..."})

        # --- CIO Simluated Streaming ---
        cio_header = "\n\n### ğŸ–ï¸ é¦–å¸­æŠ•èµ„å®˜ (CIO) æœ€ç»ˆè£å†³\n\n"
        yield json.dumps({"type": "token", "content": cio_header})
        
        # å°†ç»“æœæŒ‰ chunk åˆ‡åˆ†ï¼Œæ¯éš”ä¸€å°æ®µæ—¶é—´ yield ä¸€æ¬¡
        chunk_size = 8 # æ¯æ¬¡è¾“å‡º8ä¸ªå­—ç¬¦
        for i in range(0, len(cio_result), chunk_size):
            chunk = cio_result[i:i+chunk_size]
            yield json.dumps({"type": "token", "content": chunk})
            await asyncio.sleep(0.01) # æçŸ­çš„å»¶è¿Ÿæ¨¡æ‹Ÿæ‰“å­—æ„Ÿ
            
        cio_section = cio_header + cio_result + "\n\n"
        
        # Final formatting
        full_report = debate_content + cio_section
        
        yield json.dumps({"type": "progress", "value": 100, "message": "åˆ†æå®Œæˆ"})
        yield json.dumps({"type": "final_html", "content": full_report})
        yield json.dumps({"type": "complete", "content": "Done"})
